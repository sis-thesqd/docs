---
title: "AA26"
author: "Jacob Vendramin"
date: "2025-12-16"
last_updated: "2025-12-19"
tags: ["auto-scheduler"]
version: "1.3.1"
emoji: "üë•"
---

# üë• Auto-Scheduling

## üéØ Beginner's Guide: Quick Reference

### What Controls What

**Config Controls** (sis_config table)
- **Router behavior**: `router_config` controls classification statuses, types, limits, and narrative messages
- **Task eligibility**: `task_processing_filters` determines if a task is ready for auto-assignment
- **Designer filtering**: `min_preference_score` filters designers by account preference
- **Preference behavior**: `skip_preference_score_after` controls when preferences are ignored (>14 days)
- **Final selection**: `Decision Script Configuration` controls how the final designer is chosen
- **Output format**: `Master Output Config` controls what data is returned

**Where to Look for Different Things**

| What You Need | Where to Find It | File/Location |
|--------------|------------------|---------------|
| Modify router status strings | `router_config.statuses` config | sis_config table |
| Modify router classification sets | `router_config.classifications` config | sis_config table |
| Customize router narratives | `router_config.narratives` config | sis_config table |
| Adjust router timeouts | `router_config.limits` config | sis_config table |
| Modify task validation rules | `task_processing_filters` config | sis_config table |
| Change preference score threshold | `min_preference_score` config | sis_config table |
| Adjust selection logic | `Decision Script Configuration` config | sis_config table |
| Enable/disable dev mode | `aa26_dev_mode` config | sis_config table |
| Update business logic | Core functions | f/aa26_v2/core.py |
| Change script parameters | Script entry points | f/aa26_v2/{script}.py |
| Modify RPC queries | Supabase functions | as_get_task_processing_data, as_get_assignees_data |
| View execution logs | Database table | as_log table |
| Check designer availability | Database table | as_designer_availabilities table |

### Quick Troubleshooting

**Task not processing?**
1. Check `task_processing_filters` in sis_config
2. Verify task passes all three filters: not a subtask, no designer assigned, status not blocked
3. Check as_log table for error messages

**Wrong designer selected?**
1. Review `Decision Script Configuration` settings
2. Check if `high_usage` flag is affecting priority rules
3. Verify preference scores in account data

**Preference scores not working?**
1. Check if task is >14 days out (controlled by `skip_preference_score_after`)
2. Verify `min_preference_score` threshold
3. Confirm preference data exists in as_get_task_assignment_data

---

## üìö Complete Technical Guide

### Architecture Overview

The AA26 workflow consists of 7 core scripts that execute in a specific order:

```javascript
Master Script (orchestrator)
‚îú‚îÄ‚îÄ 1. Config Script (fetch all configs)
‚îú‚îÄ‚îÄ 2. Task Script (validate & fetch task data)
‚îú‚îÄ‚îÄ 3. Account Script (fetch designer preferences)
‚îú‚îÄ‚îÄ 4. Assignees Script (calculate availability)
‚îú‚îÄ‚îÄ 5. Decision Script (select final designer)
‚îú‚îÄ‚îÄ 6. Queue Script (move tasks to queue when no room)
‚îî‚îÄ‚îÄ 7. Assign Script (apply assignment to ClickUp) [NEW 2025-12-18]
```

**Execution Flow:**
- **Parallel Phase**: Config, Task, and Account run simultaneously
- **Sequential Phase**: Assignees waits for Task + Account, then Decision waits for all previous

---

### Router Script (f/aa26_v2/router.py)

**Purpose**: Pre-processes tasks to determine routing classification before entering the main AA26 flow.

**Configuration**: Uses `router_config` from sis_config table (added 2025-12-17)

**Key Classifications** (configurable via `classifications` in router_config):
- `auto_schedule`: Task ready for auto-scheduling (process=True)
- `queued`: Task should be added to queue because no room available (process=True) *(Updated 2025-12-17)*
- `skip`: Auto-assign override enabled (process=False)
- `process_queued_tasks`: Parent task completed, account has queued tasks waiting (process=True)
- `completed_no_queued`: Parent task completed but no queued tasks (process=False)
- `move_subtasks`: Parent assigned, subtasks unassigned (process=True)
- `move_dependent_tasks`: Deliverables needed, dependent tasks unassigned (process=True)

**Configurable Values** (via `router_config` in sis_config):
- **Status strings**: Which statuses indicate completion, need processing, or trigger dependent moves
- **Classification sets**: Which types are actionable, skip, or blocking
- **Limits**: HTTP timeout (30s default), queued task fetch limit (100 default)
- **Narrative templates**: Customizable message templates with variable substitution ({count}, {parent}, {room})

**Completed Parent Task Logic** (Added 2025-12-16):

When the input task's status is in `completed_statuses` (default: "closed", "final files delivered"):
1. Router checks `aa_queued_count` from `get_task_auto_assign_data` RPC
2. If `aa_queued_count > 0`: Returns `process=True` with classification `process_queued_tasks`
3. If `aa_queued_count = 0`: Returns `process=False` with classification `completed_no_queued`

This enables automatic processing of queued tasks when a parent task is completed, freeing up capacity on the account.

**Enriched Task ID Arrays** (Added 2025-12-17):

When the router outputs `queued_task_ids`, `subtask_ids`, or `dependent_task_ids`, these arrays are now enriched with task details:

- **task_id**: The ClickUp task ID
- **due_date**: Formatted as `YYYY-MM-DD` (null if no due date)
- **status**: Current task status (lowercase)
- **assignees**: Array of employee assignees (only those whose email exists in `designer_scheduling` table)

**Concurrent Fetching**: Task details are fetched concurrently using `asyncio.gather()` to minimize API latency. Designer emails are fetched once at startup in parallel with the main task data.

**Returns**:
```json
{
  "classifications": ["process_queued_tasks"],
  "process": true,
  "narrative": "Task 86dr3jgd6 has status 'closed' indicating completion...",
  "queued_task_ids": [
    {
      "task_id": "86dybwjk6",
      "due_date": "2025-12-20",
      "status": "open",
      "assignees": ["john.doe"]
    },
    {
      "task_id": "86dytgu78",
      "due_date": null,
      "status": "open",
      "assignees": []
    }
  ],
  "orchestration_result": {
    "processed": 2,
    "succeeded": 2,
    "failed": 0,
    "results": [
      {"task_id": "86dybwjk6", "status": "assigned", "selected_designer": "John Doe", "due_date": "2025-12-22"},
      {"task_id": "86dytgu78", "status": "assigned", "selected_designer": "Jane Smith", "due_date": "2025-12-23"}
    ]
  },
  "account_details": {
    "account": 1909,
    "cap": 12,
    "active_tasks": 10,
    "room": 2,
    "aa_queued_count": 10
  }
}
```

**Router Orchestration** (Added 2025-12-18):

The router now acts as the central orchestrator, directly calling downstream scripts instead of returning classifications for external handling:

| Classification | Orchestration Action |
|---------------|---------------------|
| `auto_schedule` | Calls master.py ‚Üí assign.py for the current task |
| `queued` | Calls queue.py with pre-provided data (avoids duplicate RPCs) |
| `process_queued_tasks` | Loops through queued_task_ids, calls master ‚Üí assign for each (limited by room) |
| `move_subtasks` | Loops through subtask_ids, calls master ‚Üí assign for each (limited by room) |
| `move_dependent_tasks` | Loops through dependent_task_ids, calls master ‚Üí assign for each (limited by room) |

**Error Handling**: Failed tasks are recorded in the `orchestration_result` but don't stop processing of remaining tasks.

**Why Enriched Arrays?**
This allows downstream consumers to know what action needs to be taken on each task without making additional API calls:
- Tasks with `status: "open"` and no assignees are ready for scheduling
- Tasks with a `due_date` may need different handling
- Tasks with `assignees` already have employee assignments

<details>
<summary><h2>üîß Script Reference</h2></summary>

### 1. Config Script

**Path**: `f/aa26_v2/config.py`

**Purpose**: Fetches all aa26 configuration from sis_config table

**Entry Point**:
```python
def main(task_id: str) -> dict
```

**Returns**:
```json
{
  "task_id": "86dyjhcya",
  "configs": [
    {
      "id": "uuid",
      "workflows": ["f/aa26/master"],
      "metadata": {"key": "value"}
    }
  ]
}
```

**Key Function**: `fetch_all_aa26_configs()` in core.py:50-66

### 2. Task Script

**Path**: `f/aa26_v2/task.py`

**Purpose**: Validates task eligibility and fetches task processing data

**Entry Point**:
```python
def main(task_id: str = None, config_data: dict = None,
         account: str = None, tag: str = None) -> dict
```

**Accepts Two Modes**:
1. **Task ID Mode**: Provide `task_id` to fetch specific task
2. **Account/Tag Mode**: Provide `account` + `tag` to query by account and tag name

**Key Config Used**: `task_processing_filters`

**Validation Filters** (applied in core.py:185-202):
1. **must_not_be_subtask**: Checks if `task.parent` is null in ClickUp API data
2. **auto_assign_status_blocked_values**: Filters out tasks with status in ["blocked", "complete", "queued"]
3. **designer_department_must_be_empty**: Ensures no assignee has "Design" in department

**Returns**:
```json
{
  "task_id": "86dyjhcya",
  "task_name": "Design church bulletin",
  "estimate": 120,
  "max_days_out": 20,
  "min_days_out": 2,
  "department": "Design Squad",
  "tag": "Bulletin",
  "project_type": "Print",
  "ready_to_process": true,
  "is_subtask": false
}
```

**Database Upsert**: Writes to `as_log` table (core.py:235-261)

**RPC Called**: `as_get_task_processing_data` or `as_tag_time_analysis`

### 3. Account Script

**Path**: `f/aa26_v2/account.py`

**Purpose**: Fetches account data including designer preferences

**Entry Point**:
```python
def main(task_id: str, config_data: dict) -> dict
```

**Key Config Used**: `min_preference_score`

**Preference Score Filtering** (account.py:31-36):
- Filters designers where `preference_score >= min_preference_score`
- Scores: 0 (not preferred), 1 (default), 2 (preferred), 3 (recommended), 4 (both)

**Returns**:
```json
{
  "task_id": "86dyjhcya",
  "account": "1234",
  "high_usage": false,
  "active_tasks": 5,
  "cap": 10,
  "room": 5,
  "designers": [
    {
      "clickup_id": "12345",
      "preference_score": 3,
      "preference_score_text": "recommended"
    }
  ]
}
```

**Database Upsert**: Writes to `as_log` table (core.py:293-318)

**RPC Called**: `as_get_task_assignment_data`

### 4. Assignees Script

**Path**: `f/aa26_v2/assignees.py`

**Purpose**: Calculates designer availability for all eligible dates

**Entry Point**:
```python
def main(task_id: str = None, config_data: dict = None,
         task_data: dict = None, account_data: dict = None,
         account: str = None, tag: str = None,
         only_output_available: bool = False) -> dict
```

**Accepts Two Modes**:
1. **Task ID Mode**: Uses task_id to fetch data
2. **Account/Tag Mode**: Uses account + tag with optional estimate/min_days/max_days overrides

**Key Configs Used**:
- `skip_preference_score_after` (preference_score_skip: 14)
- `estimate_buffer` (1.02 multiplier)
- `assignees_config` (RPC parameters)

**Processing Steps** (core.py:366-454):
1. Fetch designer availability from RPC
2. Add preference scores from account data
3. Apply preference_score_skip rule (reset to 1 if days_out > 14)
4. Calculate total_available (available_priority + available_standard)
5. Optionally filter to only available designers

**Returns**:
```json
{
  "task_id": "86dyjhcya",
  "designers": [
    {
      "designer": "John Doe",
      "email": "john@example.com",
      "days_out": 3,
      "available_priority": 120,
      "available_standard": 240,
      "total_available": 360,
      "preference_score": 3
    }
  ]
}
```

**Database Upsert**: Writes to `as_designer_availabilities` table (core.py:456-499)

**RPC Called**: `as_get_assignees_data`

### 5. Decision Script

**Path**: `f/aa26_v2/decision.py`

**Purpose**: Selects the final designer based on priority rules

**Entry Point**:
```python
def main(task_id: str, config_data: dict, task_data: dict,
         account_data: dict, assignees_data: dict) -> dict
```

**Key Config Used**: `Decision Script Configuration`

**Decision Logic** (core.py:566-656):

**Step 1: Apply Filters**
- Exclude designers with preference_score < 1 (not_preferred/not_recommended)
- Exclude designers with exclude.value = true
- Exclude designers where total_available <= task estimate

**Step 2: Sort by Priority**
1. days_out (ascending - earliest first)
2. prev_des (descending - previous designer prioritized)
3. preference_score (descending - higher preference first)
4. max_possible capacity (descending - more capacity first)

**Step 3: Limit Results**
- Limit to first `max_days_out_limit` unique days (default: 5)
- Ensure at least `min_results` designers (default: 10)
- Apply `preference_days_out_threshold`: reset preference_score to 1 for days_out >= 14

**Step 4: Select Final Designer**
1. If NOT high_usage: Select first designer with prev_des = 1
2. If NOT high_usage: Select first designer with preference_score >= 2
3. Fallback: Select first designer in sorted list

**Returns**:
```json
{
  "task_id": "86dyjhcya",
  "selected_designer": {
    "designer": "John Doe",
    "email": "john@example.com",
    "days_out": 3,
    "due_date": "2025-12-18"
  }
}
```

**Database Upsert**: Writes to `as_log` table with final selection (core.py:711-743)

### 6. Queue Script (Added 2025-12-18, Refactored 2025-12-18)

**Path**: `f/aa26_v2/queue.py`

**Purpose**: Moves tasks to queue when account has no room for auto-scheduling

**Entry Point**:
```python
def main(
    task_id: str,
    account_data: dict = None,
    task_data: dict = None,
    is_subtask: bool = None
) -> dict
```

**Parameters** (all optional except task_id):
- `task_id`: The ClickUp task ID to move to queue (required)
- `account_data`: Output from `fetch_task_assignment_data()` - avoids extra RPC call
- `task_data`: Output from `fetch_task_data()` - avoids extra RPC call
- `is_subtask`: Whether task is a subtask - avoids extra ClickUp API call

**Design Philosophy**: Reuses existing core functions - only fetches data that wasn't provided by the caller. This is critical because the queue script will be called from router/master scripts that already have this data.

**Processing Steps**:
1. Check what data was provided vs needs to be fetched
2. Fetch only missing data in parallel:
   - `fetch_non_employee_assignees()` - always fetched (queue-specific)
   - `fetch_task_assignment_data()` - only if account_data not provided
   - `fetch_task_data()` - only if task_data not provided
   - `fetch_clickup_task()` - only if is_subtask not provided
3. Calculate next queue number for the account
4. Upsert to as_log with all available fields
5. Add "queued" tag to ClickUp task
6. Send queue notification comment to each non-employee assignee

**Key Functions** (core.py):
- `fetch_non_employee_assignees()`: **Queue-specific** - filters assignees to only non-employees for comment notifications
- `fetch_task_assignment_data()`: **Reused** - gets account info (high_usage, cap, room, mhs3, etc.)
- `fetch_task_data()`: **Reused** - gets task metadata (task_name, estimate, department, tag, min/max_days_out)
- `fetch_clickup_task()`: **Reused** - gets ClickUp task to determine if subtask
- `get_queue_num_for_account()`: Counts current queued tasks + 1
- `upsert_queue_to_as_log()`: Updates as_log with all available fields
- `add_clickup_tag()`: Adds "queued" tag via ClickUp API

**Performance Optimization**:
- When called standalone: ~1165ms (fetches 4 data sources)
- When called with pre-provided data: ~812ms (fetches only 2 data sources)
- Logs what was provided vs fetched for debugging

**Data Populated in as_log**:
| Field | Source |
|-------|--------|
| task_id | Input parameter |
| task_name, estimate, department, tag | `as_get_task_processing_data` RPC |
| max_days_out, min_days_out | `as_get_task_processing_data` RPC |
| is_subtask | ClickUp API (parent check) |
| account, high_usage | `as_get_task_assignment_data` RPC |
| active_tasks, cap, room | `as_get_task_assignment_data` RPC |
| mhs3, design_tasks_last_7_days | `as_get_task_assignment_data` RPC |
| status | Set to "queued" |
| queue_num | Calculated (count + 1) |

**Comment Configuration**:
```python
QUEUE_COMMENT_ID = "08420f90-a292-43ac-ba1c-534692022f54"
QUEUE_COMMENT_REPLACE_URL = "https://requests.thesqd.com/queue"
QUEUE_COMMENT_SEARCH_TEXT = "project will be automatically added to our schedule when you go below"
QUEUE_COMMENT_DAYS_BACK = 7
```

**Returns**:
```json
{
  "status": "queued",
  "task_id": "86dyu6p54",
  "account": 1691,
  "queue_num": 1,
  "comments_sent": 1,
  "non_employee_assignees_count": 1
}
```

**Error Response**:
```json
{
  "status": "error",
  "task_id": "86dyu6p54",
  "error": "Task not found or has no account"
}
```

**Database Upsert**: Writes to `as_log` table with all available fields from both RPCs (core.py)

**Dependencies**:
- Calls `f/clickup/send-comment` via Windmill client for comment notifications
- Uses ClickUp API to add "queued" tag and check subtask status

### 7. Assign Script (Added 2025-12-18)

**Path**: `f/aa26_v2/assign.py`

**Purpose**: Applies auto-scheduler decisions to ClickUp tasks - adds assignee, sets due date, removes queue tag, sends notifications

**Entry Point**:
```python
def main(
    task_id: str,
    selected_designer: dict,
    non_employee_assignees: list = None,
    remove_queued_tag: bool = True,
    send_comments: bool = True
) -> dict
```

**Parameters**:
- `task_id`: The ClickUp task ID to assign (required)
- `selected_designer`: Output from decision.py containing:
  - `clickup_id`: Designer's ClickUp user ID
  - `date`: Selected due date (YYYY-MM-DD format)
  - `designer`: Designer name
  - `email`: Designer email
  - `days_out`: Days until due date
  - `time`: Time availability data dict
- `non_employee_assignees`: Optional - will fetch if not provided
- `remove_queued_tag`: Whether to remove "queued" tag (default: True)
- `send_comments`: Whether to send notification comments (default: True)

**Design Philosophy**: Uses modular ClickUp functions from core.py for all API operations. Accepts optional `non_employee_assignees` to avoid duplicate fetches when called from router.

**Processing Steps**:
1. Validate `selected_designer` has required fields (`clickup_id`, `date`)
2. Optionally fetch `non_employee_assignees` if not provided
3. Update ClickUp task: add assignee + set due date + set status to "received"
4. Remove "queued" tag if `remove_queued_tag=True`
5. Log update to `task_update_log` table
6. Upsert to `as_log` with status="assigned"
7. Send assignment notification comment to each non-employee assignee

**Key Functions** (core.py):
- `update_clickup_task()`: Unified task update - assignees, due_date, status via PUT endpoint
- `edit_clickup_tags()`: Modular tag add/remove via POST/DELETE endpoints
- `log_task_update()`: Insert to task_update_log table for audit trail
- `fetch_non_employee_assignees()`: Get non-employee assignees for comment notifications
- `format_due_date()`: Convert date string to ClickUp timestamp (milliseconds)

**Comment Configuration**:
```python
ASSIGN_COMMENT_ID = "3195ca79-04b1-4e1f-bc94-e7d63e405acc"
ASSIGN_COMMENT_SEARCH_TEXT = "project has been assigned to a designer"
ASSIGN_COMMENT_DAYS_BACK = 7
```

**Returns**:
```json
{
  "task_id": "86dyjhcya",
  "status": "assigned",
  "selected_designer": "John Doe",
  "assignee_id": "12345",
  "due_date": "2025-12-20",
  "days_out": 3,
  "clickup_updated": true,
  "tags_removed": ["queued"],
  "comments_sent": 1,
  "non_employee_assignees_count": 1
}
```

**Error Response**:
```json
{
  "task_id": "86dyjhcya",
  "status": "error",
  "error": "selected_designer missing clickup_id"
}
```

**Database Updates**:
- `as_log`: Upserts with status="assigned", selected_designer, due_date, days_out
- `task_update_log`: Inserts row with narrative and workflow_id for audit trail

**Dependencies**:
- Calls `f/clickup/send-comment` via Windmill client for comment notifications
- Uses ClickUp API for task updates (assignees, due_date, status)
- Uses ClickUp API for tag removal

### 8. Master Script

**Path**: `f/aa26_v2/master.py`

**Purpose**: Orchestrates all scripts and handles execution flow

**Entry Point**:
```python
def main(task_id: str = None, make_selection: bool = True,
         account: str = None, tag: str = None,
         only_output_available: bool = False) -> dict
```

**Parameters**:
- `task_id`: ClickUp task ID (optional if account/tag provided)
- `make_selection`: Run decision script (default: true)
- `account`: Account number for account/tag mode
- `tag`: Tag name for account/tag mode
- `only_output_available`: Filter to only available designers

**Execution Phases**:

**Phase 1: Parallel Fetch** (master.py:78-113)
```python
config_raw, task_raw, account_raw, clickup_raw = await asyncio.gather(
    fetch_all_aa26_configs(),
    fetch_task_data(task_id) or fetch_tag_time_analysis(account, tag),
    fetch_task_assignment_data(task_id),
    fetch_clickup_task(session, task_id)
)
```

**Phase 2: Process Task & Account** (master.py:118-227)
- Process task data with filters
- Process account data with preference filtering
- Upsert to as_log

**Phase 3: Assignees** (master.py:229-301)
- Fetch designer availability
- Add preference scores
- Apply preference_score_skip
- Calculate total_available
- Upsert to as_designer_availabilities

**Phase 4: Decision** (master.py:303-376, optional)
- Apply decision filters
- Sort and limit designers
- Select final designer
- Generate decision log
- Upsert to as_log

**Config Filtering** (master.py:28-45):
```python
def filter_config_for_script(config_data: dict, script_name: str) -> dict:
    # Merges metadata from all configs where script_name is in workflows array
```

**Returns**:
```json
{
  "success": true,
  "task_id": "86dyjhcya",
  "config": {"configs": [...]},
  "task": {...},
  "account": {...},
  "assignees": {...},
  "decision": {...},
  "execution_time_ms": 1250
}
```

**Account/Tag Mode**: Returns only assignees and decision (skips task/account/config)

**Key Config Used**: `Master Output Config` controls if config object is included

</details>

<details>
<summary><h2>üß™ Dev Mode (Added 2025-12-18)</h2></summary>

Dev Mode allows testing the auto-scheduler workflow on a dedicated test task without affecting production tasks.

### Configuration

Dev Mode is configured in `sis_config` table under `aa26_dev_mode`:

```json
{
  "dev_mode": true,
  "dev_task_id": "86dyjhcya"
}
```

### How It Works

When `dev_mode=True`:

1. **Router.py**: Uses `dev_task_id` for all operations instead of the original task_id
   ```python
   clickup_task_id = dev_task_id if (dev_mode and dev_task_id) else actual_task_id
   ```

2. **Time Estimate Flow**: Sets estimate on dev_task_id, logs with "[DEV MODE]" prefix

3. **Queue.py**: Adds tags and sends comments to dev_task_id, logs with dev status

4. **Assign.py**:
   - Updates due date and status on dev_task_id
   - Removes "queued" tag
   - **Does NOT add assignee** (key difference from production)
   - Logs with "assigned_dev" status

### Dev Mode Output Markers

All scripts include `dev_mode: true` in their output when running in dev mode:

```json
{
  "task_id": "86dyjhcya",
  "status": "assigned",
  "dev_mode": true
}
```

Log narratives are prefixed with "[DEV MODE]":
```
[DEV MODE] auto-scheduler (aa26_v2) - assigned to John Doe with due date 2024-01-15
```

### as_log Status Values

| Status | Description |
|--------|-------------|
| `in_progress` | Task is being processed |
| `queued` | Task was added to queue |
| `queued_dev` | Task was queued in dev mode |
| `assigned` | Task was assigned to a designer |
| `assigned_dev` | Task was assigned in dev mode |

</details>

<details>
<summary><h2>‚è±Ô∏è Time Estimate Flow (Added 2025-12-18)</h2></summary>

When a task is classified as `auto_schedule`, the Router sets the time estimate on both ClickUp and the database **before** looking for designer availabilities.

### Why Set Estimate Early?

1. **Visibility**: The estimate appears on the ClickUp task immediately
2. **Availability Calculation**: The estimate is needed for the assignees script to calculate which designers have capacity
3. **Audit Trail**: Logs the estimate being set for debugging/tracking

### Implementation (router.py)

```python
# Extract estimate from classification data
estimate = auto_assign_data.get("time_estimate_mins", 0)
task_name = auto_assign_data.get("name") or task.get("name", "")
account = auto_assign_data.get("account")
tag = auto_assign_data.get("tag")
department = auto_assign_data.get("resp_dept")

if estimate and estimate > 0:
    dev_prefix = "[DEV MODE] " if dev_mode else ""
    estimate_narrative = f"{dev_prefix}auto-scheduler (aa26_v2) - set time estimate to {estimate} mins"

    # Run all estimate operations in parallel
    estimate_tasks = [
        set_clickup_time_estimate(session, clickup_task_id, estimate),
        upsert_estimate_to_as_log(
            task_id=clickup_task_id,
            estimate=estimate,
            task_name=task_name,
            account=account,
            tag=tag,
            department=department
        ),
        log_task_update(clickup_task_id, "f/aa26_v2/router", estimate_narrative)
    ]

    estimate_results = await asyncio.gather(*estimate_tasks, return_exceptions=True)
```

### Core Functions (core.py)

**set_clickup_time_estimate()**: Sets time estimate on ClickUp task via API
```python
async def set_clickup_time_estimate(session, task_id, estimate_mins) -> dict:
    # ClickUp expects time_estimate in milliseconds
    estimate_ms = estimate_mins * 60 * 1000

    url = f"{CLICKUP_BASE_URL}/task/{task_id}"
    payload = {"time_estimate": estimate_ms}

    async with session.put(url, headers=CLICKUP_HEADERS, json=payload) as response:
        # ... handle response
```

**upsert_estimate_to_as_log()**: Saves estimate to as_log table
```python
async def upsert_estimate_to_as_log(task_id, estimate, task_name, account, tag, department) -> bool:
    upsert_data = {
        "task_id": task_id,
        "estimate": estimate,
        "status": "in_progress",
        "row_updated": "now()"
    }
    # ... upsert to as_log
```

### Parallel Execution Benefits

Running these three operations in parallel:
- **~200ms** for ClickUp API call
- **~50ms** for database upsert
- **~50ms** for log entry

Total: **~200ms** (not ~300ms if sequential) - saves ~100ms per task

</details>

<details>
<summary><h2>‚öôÔ∏è Configuration Reference (sis_config)</h2></summary>

### router_config (Added 2025-12-17)

**Workflows**: f/aa26_v2/router

**Metadata**:
```json
{
  "statuses": {
    "completed": ["closed", "final files delivered"],
    "needs_processing": "open",
    "deliverables_trigger": "deliverables needed"
  },
  "classifications": {
    "actionable": ["auto_schedule", "move_dependent_tasks", "move_subtasks"],
    "skip": ["skip", "queued"],
    "blocking": [
      "has_employee_assignees",
      "has_subtasks",
      "has_dependencies",
      "is_subtask",
      "is_dependent_on_another"
    ]
  },
  "limits": {
    "http_timeout_seconds": 30,
    "queued_task_fetch_limit": 100
  },
  "narratives": {
    "has_employee_assignees": "task already has employee assignees",
    "has_subtasks": "task has {count} subtask(s) but none need processing",
    "move_subtasks": "parent is assigned and {count} subtask(s) need scheduling",
    "has_dependencies": "other tasks depend on this one but none need processing",
    "move_dependent_tasks": "status is 'deliverables needed' and {count} dependent task(s) need scheduling",
    "is_subtask": "task is a subtask (parent: {parent})",
    "is_dependent_on_another": "task depends on another task that must complete first",
    "auto_schedule": "task is ready for auto-scheduling with no blocking conditions",
    "queued": "task should be queued because account has no room",
    "skip": "auto-assign override is enabled for this account",
    "decision_process_true_auto": "Decision: process=True because task is ready for auto-scheduling.",
    "decision_process_true_subtasks": "Decision: process=True because subtasks need to be scheduled.",
    "decision_process_true_deps": "Decision: process=True because dependent tasks need to be scheduled.",
    "decision_process_false_skip": "Decision: process=False because auto-assign override is enabled.",
    "decision_process_false_queued": "Decision: process=False because task is queued (no room available).",
    "decision_process_false_blocking": "Decision: process=False because blocking conditions exist that prevent auto-scheduling.",
    "decision_room_override": "Decision: process=False because account has no room (room={room}), even though task would otherwise be processed."
  }
}
```

**Usage**:
- **statuses.completed**: Statuses that indicate task completion (triggers queue check)
- **statuses.needs_processing**: Status required for subtasks/dependent tasks to be processed (default: "open")
- **statuses.deliverables_trigger**: Status that triggers `move_dependent_tasks` classification
- **classifications.actionable**: Classification types that set `process=True`
- **classifications.skip**: Classification types that halt processing
- **classifications.blocking**: Classification types that indicate blocking conditions
- **limits.http_timeout_seconds**: HTTP client timeout for ClickUp API calls
- **limits.queued_task_fetch_limit**: Maximum queued tasks to fetch (safety cap)
- **narratives.\***: Template strings for generating human-readable decision explanations. Variables: `{count}`, `{parent}`, `{room}`

**Benefits**:
- Add new statuses without code changes
- Customize narrative messages for different contexts
- Tune timeouts for performance
- Add new classification types dynamically

---

### aa26_global_constants

**Workflows**: f/aa26/master, f/aa26/task, f/aa26/assignees

**Metadata**:
```json
{
  "blocked_domain": "external.com",
  "request_timeout": 30,
  "valid_space_ids": ["12345", "67890"],
  "default_max_days_out": 20,
  "default_min_days_out": 2
}
```

**Usage**:
- `request_timeout`: HTTP client timeout for ClickUp API (task.py:46)
- `blocked_domain`: Prevents external assignees from blocking auto-assignment
- `valid_space_ids`: Eligible ClickUp spaces for processing
- `default_max_days_out` / `default_min_days_out`: Date range window defaults

---

### min_preference_score

**Workflows**: f/aa26/account

**Metadata**:
```json
{
  "min_preference_score": 2
}
```

**Usage**: Filters designers in account.py:31-36 and master.py:204-209
- Only keeps designers with preference_score >= 2
- Removes default (1) and not_preferred (0) designers

---

### task_processing_filters

**Workflows**: f/aa26/task

**Metadata**:
```json
{
  "ready_to_process_filters": {
    "must_not_be_subtask": true,
    "assignee_must_not_contain": "Design",
    "auto_assign_status_blocked_values": ["blocked", "complete", "queued"]
  }
}
```

**Usage**: Applied in core.py:185-202
- Validates task eligibility for auto-assignment
- Task must pass ALL filters to be ready_to_process

---

### skip_preference_score_after

**Workflows**: f/aa26/assignees

**Metadata**:
```json
{
  "preference_score_skip": 14
}
```

**Usage**: Applied in core.py:392-415
- For designers with days_out > 14, resets preference_score to 1
- Sets preference_score_text to "days_out_override"
- Ensures urgency takes precedence over preferences for distant dates

---

### assignees_config

**Workflows**: f/aa26/assignees

**Metadata**:
```json
{
  "result_limit": 100,
  "default_max_days": 20,
  "default_min_days": 2,
  "default_department": "Design Squad",
  "priority_buffer_ratio": 0.3,
  "check_subtask_via_clickup": true
}
```

**Usage**: Used by as_get_assignees_data RPC function
- Controls RPC query parameters and filtering logic

---

### estimate_buffer

**Workflows**: f/aa26/assignees

**Metadata**:
```json
{
  "estimate_buffer": 1.02
}
```

**Usage**: Used by as_get_assignees_data RPC function
- Multiplies estimate by 1.02 before availability comparison
- Provides 2% buffer for estimation variance

---

### Decision Script Configuration

**Workflows**: f/aa26/decision

**Metadata**:
```json
{
  "high_usage": false,
  "min_results": 10,
  "max_days_out_limit": 5,
  "availability_output": "single",
  "preference_days_out_threshold": 14
}
```

**Usage**:
- `high_usage`: Skips prev_des and preference_score priority rules (core.py:644-656)
- `min_results`: Minimum designers to return (core.py:611-628)
- `max_days_out_limit`: Limit to first X unique days (core.py:609)
- `availability_output`: "single" selects one, "all" returns all (decision.py:35, master.py:319)
- `preference_days_out_threshold`: Reset scores for days_out >= 14 (core.py:611-618)

---

### Master Output Config

**Workflows**: f/aa26/master

**Metadata**:
```json
{
  "output_config_data": false
}
```

**Usage**: Controls config inclusion in master.py:390-424
- When false, excludes config object from response (reduces payload size)
- When true, includes full config array

---

### aa26_dev_mode (Added 2025-12-18)

**Workflows**: f/aa26_v2/router

**Metadata**:
```json
{
  "dev_mode": true,
  "dev_task_id": "86dyjhcya"
}
```

**Usage**: Controls development/testing mode
- `dev_mode`: When true, redirects all operations to dev_task_id
- `dev_task_id`: The ClickUp task ID to use for testing

**Important**: When dev_mode is enabled:
- All ClickUp API operations (time estimate, tags, status) happen on dev_task_id
- All database operations (as_log, task_update_log) reference dev_task_id
- Assign script does NOT add designer as assignee (prevents affecting real designer workload)
- All log entries are prefixed with "[DEV MODE]"

---

### Timeoff Patterns Configuration

**Workflows**: f/aa26/assignees

**Metadata**:
```json
{
  "timeoff_patterns": {
    "full_day": {
      "description": "Full day off - sets leave to max capacity and availability to 0",
      "amount_values": ["all_of_the_day"],
      "workdays_off_threshold": 1
    },
    "half_day": {
      "description": "Half day off - sets leave to half capacity and cuts availability in half",
      "amount_values": ["half_of_the_day"],
      "workdays_off_equals": 1
    },
    "hours_based": {
      "description": "Hours-based timeoff - parses hours from amount field and reduces availability accordingly",
      "amount_pattern": "%hours%",
      "workdays_off_equals": 1
    }
  }
}
```

**Usage**: Used by as_get_assignees_data RPC function
- Parses time-off records from timeoff data source
- Calculates leave and reduces availability accordingly

</details>

<details>
<summary><h2>üóÑÔ∏è Database Tables</h2></summary>

### as_log

**Purpose**: Tracks execution history and final decisions for each task

**Key Columns**:
- `task_id` (primary key)
- `task_name`, `estimate`, `department`, `tag`
- `account`, `high_usage`, `active_tasks`
- `selected_designer`, `assignee_id`, `selected_due_date`
- `narrative` (decision log message)
- `total_options_count`
- `status` (in_progress ‚Üí completed ‚Üí queued)
- `queue_num` (position in queue for account, added 2025-12-18)

**Populated By**:
- Task script: task metadata
- Account script: account metadata
- Decision script: final selection and narrative
- Queue script: queue status and queue_num

---

### as_designer_availabilities

**Purpose**: Stores designer availability snapshots for each task execution

**Key Columns**:
- `task_id`, `clickup_id`, `date` (composite key)
- `designer`, `email`, `department`
- `days_out`
- `available_priority`, `available_standard`
- `max_possible`, `scheduled`, `blocked`, `leave`
- `exclude_reason`, `exclude_value`
- `preference_score`, `preference_score_text`

**Populated By**: Assignees script (core.py:456-499)

---

### task_update_log (Added 2025-12-18)

**Purpose**: Audit trail for all ClickUp task modifications made by AA26 scripts

**Key Columns**:
- `row_id` (uuid, auto-generated)
- `created_at` (timestamp, auto-generated)
- `task_id` (text, required)
- `workflow_id` (text, script path e.g., "f/aa26_v2/assign")
- `server` (enum, "windmill")
- `narrative` (text, human-readable description of change)

**Populated By**: Assign script via `log_task_update()` function (core.py)

**Example Entry**:
```json
{
  "task_id": "86dyjhcya",
  "workflow_id": "f/aa26_v2/assign",
  "server": "windmill",
  "narrative": "auto-scheduler (aa26_v2) - assigned to John Doe with due date 2025-12-20"
}
```

</details>

<details>
<summary><h2>üîß Core Functions Reference (Added 2025-12-18)</h2></summary>

### ClickUp Functions (core.py)

These modular functions provide unified ClickUp API integration for all AA26 scripts.

#### format_due_date()
```python
def format_due_date(date_input) -> int
```
Converts date string or datetime to ClickUp timestamp (milliseconds).
- Accepts: int (passthrough), str ("YYYY-MM-DD" or ISO format), datetime object
- Returns: Unix timestamp in milliseconds with time set to 18:00:00

#### update_clickup_task()
```python
async def update_clickup_task(
    session: aiohttp.ClientSession,
    task_id: str,
    assignees_add: list = None,   # [user_id, ...]
    assignees_rem: list = None,   # [user_id, ...]
    due_date = None,              # "2025-12-20" or timestamp
    status: str = None            # "received", "open", etc.
) -> dict
```
Unified task update - only sends provided args in API payload.
- Uses PUT `/task/{task_id}` endpoint
- Dynamically builds payload from non-None arguments
- Returns ClickUp API response

**Example Usage**:
```python
result = await update_clickup_task(
    session=session,
    task_id="86dyjhcya",
    assignees_add=[12345678],
    due_date="2025-12-20",
    status="received"
)
```

#### edit_clickup_tags()
```python
async def edit_clickup_tags(
    session: aiohttp.ClientSession,
    task_id: str,
    tags_add: list = None,   # ["rush", "priority"]
    tags_rem: list = None    # ["queued"]
) -> dict
```
Add and/or remove tags from a ClickUp task.
- Uses POST `/task/{task_id}/tag/{tag}` for adding
- Uses DELETE `/task/{task_id}/tag/{tag}` for removing
- Returns summary: `{"added": [], "removed": [], "errors": []}`

**Example Usage**:
```python
result = await edit_clickup_tags(
    session=session,
    task_id="86dyjhcya",
    tags_add=["rush"],
    tags_rem=["queued"]
)
# Returns: {"added": ["rush"], "removed": ["queued"], "errors": []}
```

#### log_task_update()
```python
async def log_task_update(
    task_id: str,
    workflow_id: str,   # "f/aa26_v2/assign"
    narrative: str      # Human-readable description
) -> bool
```
Logs task modification to `task_update_log` table for audit trail.
- Inserts row with task_id, workflow_id, server="windmill", narrative
- Returns True on success

**Example Usage**:
```python
await log_task_update(
    task_id="86dyjhcya",
    workflow_id="f/aa26_v2/assign",
    narrative="auto-scheduler (aa26_v2) - assigned to John Doe with due date 2025-12-20"
)
```

</details>

<details>
<summary><h2>üîå Supabase RPC Functions</h2></summary>

### as_get_task_processing_data

**Parameters**: `input_task_id`

**Returns**: Task metadata including estimate, tags, dependencies, assignees, status

**Called By**: Task script (core.py:73-87)

---

### as_tag_time_analysis

**Parameters**: `input_account_number`, `input_tag_name`

**Returns**: Historical time data for account/tag combination

**Called By**: Task script in account/tag mode (core.py:90-112)

---

### as_get_task_assignment_data

**Parameters**: `input_task_id`

**Returns**: Account data with designer preferences and account metadata

**Called By**: Account script (core.py:268-282)

---

### as_get_assignees_data

**Parameters**:
- `input_task_id` (optional)
- `input_account` (optional)
- `input_tag` (optional)
- `input_estimate` (optional)
- `input_min_days` (optional)
- `input_max_days` (optional)

**Returns**: Designer availability data for date range

**Called By**: Assignees script (core.py:325-363)

**Config Integration**: Uses `assignees_config`, `estimate_buffer`, `Timeoff Patterns Configuration`

**Important Note (2025-12-19)**: The simple version `as_get_assignees_data(input_task_id text)` was dropped to resolve a PostgreSQL function overloading conflict. Only the complex version with all optional parameters now exists. Backup saved to `/rpc/as_get_assignees_data_simple_version_backup_2024_12_19.sql`.

</details>

<details>
<summary><h2>üöÄ API Usage Examples</h2></summary>

### Execute Full Workflow
```bash
curl -X POST https://api.windmill.dev/w/sis/jobs/run/f/aa26_v2/master \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"task_id": "86dyjhcya"}'
```

### Execute with Account/Tag Mode
```bash
curl -X POST https://api.windmill.dev/w/sis/jobs/run/f/aa26_v2/master \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"account": "1234", "tag": "Bulletin"}'
```

### Get Availability Only (No Selection)
```bash
curl -X POST https://api.windmill.dev/w/sis/jobs/run/f/aa26_v2/master \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"task_id": "86dyjhcya", "make_selection": false}'
```

### Move Task to Queue (Added 2025-12-18, Refactored 2025-12-18)
```bash
# Standalone (fetches all data)
curl -X POST https://api.windmill.dev/w/sis/jobs/run/f/aa26_v2/queue \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"task_id": "86dyu6p54"}'

# With pre-provided data (avoids duplicate RPC calls)
curl -X POST https://api.windmill.dev/w/sis/jobs/run/f/aa26_v2/queue \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "task_id": "86dyu6p54",
    "account_data": {"account": 306, "high_usage": false, "cap": 10, "room": 0},
    "is_subtask": false
  }'
```

### Apply Assignment to Task (Added 2025-12-18)
```bash
# Assign a designer to a task (uses decision.py output format)
curl -X POST https://api.windmill.dev/w/sis/jobs/run/f/aa26_v2/assign \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "task_id": "86dyjhcya",
    "selected_designer": {
      "clickup_id": "12345678",
      "date": "2025-12-20",
      "designer": "John Doe",
      "email": "john@example.com",
      "days_out": 3,
      "time": {"total_available": 360}
    }
  }'

# With pre-provided non_employee_assignees (avoids duplicate fetch)
curl -X POST https://api.windmill.dev/w/sis/jobs/run/f/aa26_v2/assign \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "task_id": "86dyjhcya",
    "selected_designer": {"clickup_id": "12345678", "date": "2025-12-20"},
    "non_employee_assignees": [{"clickup_id": "client123", "username": "Client User"}],
    "remove_queued_tag": true,
    "send_comments": true
  }'

# Assignment without sending comments
curl -X POST https://api.windmill.dev/w/sis/jobs/run/f/aa26_v2/assign \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "task_id": "86dyjhcya",
    "selected_designer": {"clickup_id": "12345678", "date": "2025-12-20"},
    "send_comments": false
  }'
```

</details>

<details>
<summary><h2>üéì Common Scenarios</h2></summary>

### Scenario 1: Modify Task Eligibility Rules

**Goal**: Add a new filter to exclude tasks with specific tags

**Steps**:
1. Update `task_processing_filters` config in sis_config table
2. Add new filter function in core.py (around line 152-182)
3. Call new filter in `apply_task_filters()` function
4. Test with sample tasks

---

### Scenario 2: Change Preference Score Threshold

**Goal**: Only consider highly preferred designers (score >= 3)

**Steps**:
1. Update `min_preference_score` config:
```sql
UPDATE sis_config
SET metadata = jsonb_set(metadata, '{min_preference_score}', '3')
WHERE name = 'min_preference_score';
```
2. No code changes needed - config is read dynamically

---

### Scenario 3: Adjust Days-Out Threshold for Preference Reset

**Goal**: Reset preferences at 10 days instead of 14

**Steps**:
1. Update `skip_preference_score_after` config:
```sql
UPDATE sis_config
SET metadata = jsonb_set(metadata, '{preference_score_skip}', '10')
WHERE name = 'skip_preference_score_after';
```
2. No code changes needed

---

### Scenario 4: Debug Why a Task Wasn't Processed

**Steps**:
1. Check as_log table for task_id
2. Look for `ready_to_process: false`
3. Check task filters in core.py:185-202
4. Verify task data meets all criteria:
   - Not a subtask
   - Status not in blocked values
   - No designer already assigned

---

### Scenario 5: Understand Why a Specific Designer Was Selected

**Steps**:
1. Check as_log.narrative for decision log message
2. Review decision priority rules in core.py:633-656
3. Check if prev_des = 1 or preference_score >= 2
4. Verify high_usage setting in Decision Script Configuration

---

### Scenario 6: Move Task to Queue (Added 2025-12-18, Refactored 2025-12-18)

**Goal**: Add a task to the queue when the account has no room

**Option A: Standalone Call** (fetches all data):
```bash
wmill script run f/aa26_v2/queue --data '{"task_id": "86dyu6p54"}'
```

**Option B: From Router/Master** (with pre-provided data to avoid duplicate calls):
```bash
wmill script run f/aa26_v2/queue --data '{
  "task_id": "86dyu6p54",
  "account_data": {"account": 306, "high_usage": false, "cap": 10, "room": 0},
  "task_data": {"task_name": "Design project", "estimate": 120},
  "is_subtask": false
}'
```

**What Happens**:
1. Script checks what data was provided vs needs to be fetched
2. Only fetches missing data (always fetches non_employee_assignees - this is queue-specific)
3. Calculates `queue_num` (count of queued tasks for that account + 1)
4. Upserts to `as_log` with `status = "queued"` and `queue_num`
5. Adds "queued" tag to the ClickUp task
6. Sends queue notification comment to each non-employee assignee

**Performance Benefit**: When called with pre-provided data, execution time drops from ~1165ms to ~812ms

**Verifying Queue Status**:
```sql
SELECT task_id, account, status, queue_num, row_updated
FROM as_log
WHERE account = 1691 AND status = 'queued'
ORDER BY queue_num;
```

---

### Scenario 7: Assign Designer to Task (Added 2025-12-18)

**Goal**: Apply auto-scheduler decision to a ClickUp task

**Option A: Full Assignment Flow** (with comments):
```bash
wmill script run f/aa26_v2/assign --data '{
  "task_id": "86dyjhcya",
  "selected_designer": {
    "clickup_id": "12345678",
    "date": "2025-12-20",
    "designer": "John Doe",
    "email": "john@example.com",
    "days_out": 3
  }
}'
```

**Option B: Quick Assignment** (skip comments):
```bash
wmill script run f/aa26_v2/assign --data '{
  "task_id": "86dyjhcya",
  "selected_designer": {"clickup_id": "12345678", "date": "2025-12-20"},
  "send_comments": false
}'
```

**What Happens**:
1. Validates `selected_designer` has `clickup_id` and `date`
2. Updates ClickUp task: adds assignee, sets due date, sets status to "received"
3. Removes "queued" tag if present
4. Logs to `task_update_log` table for audit trail
5. Updates `as_log` with status="assigned" and selection details
6. Sends assignment notification comment to non-employee assignees (if enabled)

**Verifying Assignment**:
```sql
SELECT task_id, status, selected_designer, assignee_id, selected_due_date, row_updated
FROM as_log
WHERE task_id = '86dyjhcya';
```

**Checking Audit Trail**:
```sql
SELECT task_id, workflow_id, narrative, created_at
FROM task_update_log
WHERE task_id = '86dyjhcya'
ORDER BY created_at DESC;
```

</details>

<details>
<summary><h2>üìä Performance Optimization</h2></summary>

### Parallel Execution

Master script runs config, task, and account fetches in parallel (master.py:78-113):
```python
await asyncio.gather(
    fetch_all_aa26_configs(),
    fetch_task_data(task_id),
    fetch_task_assignment_data(task_id),
    fetch_clickup_task(session, task_id)
)
```

**Benefit**: Reduces execution time by ~40%

---

### Connection Pooling

Core.py uses persistent PostgREST clients (core.py:29-43):
```python
POSTGREST_READ_CLIENT = AsyncPostgrestClient(...)
POSTGREST_WRITE_CLIENT = AsyncPostgrestClient(...)
```

**Benefit**: Reuses connections across requests

---

### Batch Upserts

Designer availabilities are upserted in batches of 100 (core.py:489-494):
```python
batch_size = 100
for i in range(0, len(records), batch_size):
    batch = records[i:i + batch_size]
    await POSTGREST_WRITE_CLIENT.from_("as_designer_availabilities").upsert(batch)
```

**Benefit**: Reduces database round trips

</details>
